# IbasePipelinePDFStages

**ETL Pipeline for Building Plan Document AI**

A 4-stage PDF processing pipeline for extracting, transforming, and storing building plan document data with machine learning models.

## ğŸ“‹ Overview

This project implements an end-to-end ETL (Extract, Transform, Load) pipeline for processing building plan PDFs, extracting structured information, and enabling AI-powered document analysis.

## ğŸ—ï¸ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1    â”‚â”€â”€â”€â”€â–¶â”‚   Stage 2    â”‚â”€â”€â”€â”€â–¶â”‚   Stage 3     â”‚â”€â”€â”€â”€â–¶â”‚ Stage 4  â”‚
â”‚  Ingestion  â”‚     â”‚  Extraction  â”‚     â”‚ Normalization â”‚     â”‚ Storage  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                    â”‚                     â”‚                    â”‚
   PDF Files         Text/Tables          Clean Data           Database
```

## ğŸ“‚ Project Structure

```
IbasePipelinePDFStages/
â”‚
â”œâ”€â”€ data/                  # Sample input PDFs (anonymized/dummy data)
â”‚   â”œâ”€â”€ raw/              # Original PDF files
â”‚   â””â”€â”€ processed/        # Intermediate outputs
â”‚
â”œâ”€â”€ ingestion/            # Stage 1: Data Ingestion
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pdf_loader.py     # Load and validate PDF files
â”‚   â””â”€â”€ batch_processor.py # Batch processing utilities
â”‚
â”œâ”€â”€ extraction/           # Stage 2: PDF Parsing and Extraction
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ text_extractor.py  # Extract text using PyMuPDF/pdfplumber
â”‚   â”œâ”€â”€ table_extractor.py # Extract tables and structured data
â”‚   â””â”€â”€ ocr_processor.py   # OCR for scanned documents
â”‚
â”œâ”€â”€ normalization/        # Stage 3: Transformation and Cleaning
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_cleaner.py    # Clean and standardize extracted data
â”‚   â”œâ”€â”€ entity_mapper.py   # Map entities (addresses, dimensions, etc.)
â”‚   â””â”€â”€ validator.py       # Data validation rules
â”‚
â”œâ”€â”€ storage/              # Stage 4: Database and File Storage
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db_handler.py      # Database operations (SQLAlchemy)
â”‚   â”œâ”€â”€ schema.py          # Database schema definitions
â”‚   â””â”€â”€ file_manager.py    # File storage management
â”‚
â”œâ”€â”€ models/               # ML Models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layout_detector.py # LayoutLM/LayoutParser for document layout
â”‚   â”œâ”€â”€ classifier.py      # Document type classification
â”‚   â””â”€â”€ ner_extractor.py   # Named Entity Recognition
â”‚
â”œâ”€â”€ utils/                # Helper Functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ logger.py          # Logging utilities
â”‚   â””â”€â”€ helpers.py         # Common utility functions
â”‚
â”œâ”€â”€ notebooks/            # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_exploration.ipynb
â”‚   â”œâ”€â”€ 02_extraction_demo.ipynb
â”‚   â””â”€â”€ 03_model_training.ipynb
â”‚
â”œâ”€â”€ tests/                # Unit Tests
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_extraction.py
â”‚   â””â”€â”€ test_normalization.py
â”‚
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .gitignore           # Git ignore patterns
â”œâ”€â”€ README.md            # This file
â””â”€â”€ main.py              # Main pipeline orchestrator
```

## ğŸ”„ Pipeline Stages

### Stage 1: Ingestion
**Purpose**: Load and validate PDF documents

- Accept PDF files from various sources
- Validate file formats and integrity
- Organize files for processing
- Handle batch uploads

**Key Files**: `ingestion/pdf_loader.py`, `ingestion/batch_processor.py`

### Stage 2: Extraction
**Purpose**: Parse PDFs and extract raw data

- Extract text content using PyMuPDF and pdfplumber
- Extract tables and structured elements
- OCR for scanned documents using Tesseract
- Detect document layout using LayoutParser/LayoutLM

**Key Files**: `extraction/text_extractor.py`, `extraction/table_extractor.py`

### Stage 3: Normalization
**Purpose**: Clean, transform, and standardize data

- Clean extracted text (remove noise, fix formatting)
- Map entities (addresses, measurements, property details)
- Standardize data formats
- Validate against business rules
- Apply domain-specific transformations

**Key Files**: `normalization/data_cleaner.py`, `normalization/entity_mapper.py`

### Stage 4: Storage
**Purpose**: Persist processed data

- Store in relational database (PostgreSQL/SQLite)
- Maintain document metadata
- Create searchable indexes
- Archive original PDFs
- Enable data retrieval APIs

**Key Files**: `storage/db_handler.py`, `storage/schema.py`

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- pip
- Tesseract OCR (for scanned PDFs)

### Setup

```bash
# Clone the repository
git clone https://github.com/bsuraj23/IbasePipelinePDFStages.git
cd IbasePipelinePDFStages

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Tesseract (if using OCR)
# Ubuntu/Debian:
sudo apt-get install tesseract-ocr

# macOS:
brew install tesseract

# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki
```

## ğŸ“¦ Dependencies

See `requirements.txt` for full list. Key libraries:

- **PDF Processing**: pdfplumber, PyMuPDF, pytesseract
- **ML/AI**: layoutparser, torch, transformers, spacy
- **Data Processing**: pandas, numpy
- **Database**: sqlalchemy, psycopg2
- **Computer Vision**: opencv-python, Pillow

## ğŸ’» Usage

### Running the Full Pipeline

```python
from main import run_pipeline

# Process a single PDF
run_pipeline("data/raw/sample_building_plan.pdf")

# Process multiple PDFs
run_pipeline("data/raw/", batch=True)
```

### Running Individual Stages

```python
# Stage 1: Ingestion
from ingestion.pdf_loader import PDFLoader
loader = PDFLoader()
pdfs = loader.load_directory("data/raw/")

# Stage 2: Extraction
from extraction.text_extractor import TextExtractor
extractor = TextExtractor()
text = extractor.extract("sample.pdf")

# Stage 3: Normalization
from normalization.data_cleaner import DataCleaner
cleaner = DataCleaner()
clean_data = cleaner.clean(text)

# Stage 4: Storage
from storage.db_handler import DBHandler
db = DBHandler()
db.save(clean_data)
```

## ğŸ¤– Machine Learning Models

### Document Layout Detection
- Uses LayoutParser with LayoutLM models
- Detects text blocks, tables, images, headers
- Extracts spatial relationships

### Document Classification
- Classifies building plan types (floor plans, elevations, sections)
- Uses fine-tuned BERT/DistilBERT models

### Named Entity Recognition
- Extracts entities: addresses, dimensions, room names, materials
- Custom NER models trained on building domain

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific stage tests
pytest tests/test_extraction.py

# Run with coverage
pytest --cov=. tests/
```

## ğŸ“Š Performance Metrics

- **Throughput**: ~50-100 PDFs per minute (depends on complexity)
- **Accuracy**: 95%+ text extraction, 85%+ table extraction
- **Latency**: <2 seconds per page (without ML models)

## ğŸ”§ Configuration

Edit `utils/config.py` or create `.env` file:

```env
DATABASE_URL=postgresql://user:password@localhost/building_plans
OCR_LANGUAGE=eng
BATCH_SIZE=10
LOG_LEVEL=INFO
```

## ğŸ“ Data Schema

See `storage/schema.py` for complete database schema.

**Key Tables**:
- `documents`: Metadata for uploaded PDFs
- `extracted_text`: Raw extracted text
- `entities`: Extracted structured entities
- `processed_data`: Final cleaned and normalized data

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“„ License

MIT License

## ğŸ‘¤ Author

**Suraj** - AI Engineer & Data Science Trainer  
GitHub: [@bsuraj23](https://github.com/bsuraj23)

## ğŸ¯ Use Cases

- **Real Estate**: Automate property document processing
- **Architecture Firms**: Digital archiving of building plans
- **Government**: Building permit automation
- **Construction**: Project document management

## ğŸš§ Roadmap

- [ ] Add support for AutoCAD DWG files
- [ ] Implement vector embedding for semantic search
- [ ] Add REST API for document upload/query
- [ ] Integrate with cloud storage (S3, Azure Blob)
- [ ] Build web UI for visualization
- [ ] Add support for 3D building models (BIM)

## ğŸ“š Resources

- [LayoutParser Documentation](https://layout-parser.github.io/)
- [pdfplumber Guide](https://github.com/jsvine/pdfplumber)
- [PyMuPDF Tutorial](https://pymupdf.readthedocs.io/)

---

**Note**: This is a demonstration project. Sample data and dummy PDFs are included for illustration purposes only.
